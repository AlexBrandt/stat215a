\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}


\begin{document}

\title{Lab 3 - Parallelizing k-means Stat 215A, Fall 2017}

\author{SID: 24092167}

\maketitle

<<Preamble, echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, quitely=TRUE, results='hide', fig.width=7, fig.height=5, fig.align='center'>>=

# Load our necessary packages.

library(dplyr)
library(foreach)
library(doParallel)
library(ggplot2)
library(grDevices)
library(reshape2)
library(ggpubr)
library(microbenchmark)
load("data/lingBinary.RData")
@

\section{Comparing C++ and R Versions of the Similarity Matrix}

Before we proceed to tackle the first part of this lab, let us first
take a brief detour to the second part of this lab, comparing the C++
and R versions of the similarity measure introduced by Fowlkes and Mallows.
This similarity matrix takes the form:

\[ cor(L_1, L_2) = \frac{\langle L_1, L_2 \rangle}{\sqrt{\langle L_1, L_1 \rangle \langle L_2, L_2 \rangle}}\]

Given that:

\[\langle L_1, L_2 \rangle = \langle C^{(1)},  C^{(2)}\rangle = \sum_{i,j}C_{ij}^{(1)},  C_{ij}^{(2)} \]

For matrices C with components:

\[
    C_ij= 
\begin{cases}
    1, & \text{if } x_i \text{ and } x_j \text{ belong to the same cluster, and } i \ne j\\
    0, & \text{otherwise}
\end{cases}
\]

These computations could become very expensive in terms of both
memory space and processor time if computed literally.  Instead, given
our label matrices \(L_1\) and \(L_2\), and their corresponding C matrices,
let us save time by computer our correlation as:

\[ cor(L_1, L_2) = \cfrac{\sum_{i,j}C_{ij}^{(1)},  C_{ij}^{(2)}}{\sum_{i,j}C_{ij}^{(1)} C_{ij}^{(1)} \sum_{i,j}C_{ij}^{(2)} C_{ij}^{(2)}} \]

Now, not only do we prevent storing the entire \(\langle L_1, L_2 \rangle\) object in memory,
but it lends itself to the simple boolean/arithmetic operations for which C++ is especially
well suited.  But, for the same of argument, let's write this space-simplified version
of the computation for both R and C++ and compare the results.

<<Timing, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, quitely=TRUE, results='hide', fig.width=7, fig.height=5, fig.align='center'>>=

# Set our global variables given the problem statement
KMAX = 10
N = 100
M <- .5

# Load our C++ code
library('Rcpp')
sourceCpp('rcpp/KmeansSimilarity.cpp')

# Gives the similarity inner product between
# two labelings -- an R implementation of the 
# first method (LabelingIP) presented in the
# C++ file.

get_similarity_score <- function(mcv1, mcv2) {
  q <- length(mcv1)
  my_sum = 0
  for (i in 1:(q-1)) {
    for(j in (i+1):q) {
      my_sum = my_sum + 
        ((mcv1[i] == mcv1[j]) * (mcv2[i] == mcv2[j]))
    }
  }
  return(2 * my_sum)
}

# Gives correlations score between two labeling
# vectors in a non-matrix fashion (i.e., uses a
# running sum).  An R implementation of the second
# method (SimilarityMeasure) presented in the C++ file.

get_nm_correlation_score <- function(l1, l2) {
  
  L.12 <- get_similarity_score(l1, l2)
  L.11 <- get_similarity_score(l1, l1)
  L.22 <- get_similarity_score(l2, l2)
  
  C.L12 <- L.12 / sqrt(L.11 * L.22)
  return(C.L12)
}

# Create two random labelings for a hypothetical
# 3-means clustering with 5000 labels (given by
# the problem statement).

t.l1 <- sample(1:3, 5000, replace = TRUE)
t.l2 <- sample(1:3, 5000, replace = TRUE)

# Test our R version vs. our C++ version using
# microbenchmark

mb <- microbenchmark(get_nm_correlation_score(t.l1, t.l2),
               SimilarityMeasure(t.l1, t.l2),
               times = 10)

# Graph the distribution of the microbenchmark results 
# to visualize the different in timing.
autoplot(mb) + 
  ggtitle("Timing Results") +
  scale_x_discrete(labels=c("SimilarityMeasure(t.l1, t.l2)" = "C++", 
                            "get_nm_correlation_score(t.l1, t.l2)" = "R")) +
  xlab("Language")
  
@

From this graph, we can see that C++ is over two orders of magnitude 
faster than R for almost the exact same code implementation.  Given 
these results, it should be fairly obvious which version
of the algorithm we wish to run!  While many of the SCP nodes were
made available for this lab, using \texttt{foreach} and my C++ function, I was 
able to run my analysis for \(m = 0.5\), \(N = 100\), and \(k_{MAX} = 10\)
on my 2013 MacBook Pro in under an hour.  Note that this would also work 
perfectly well on the SCP.

\section{Varied K Value Histograms and Cumulative Plot}
<<Fig3, echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, quitely=TRUE, results='hide', fig.width=7, fig.height=5, fig.align='center'>>=

# Load the results created from GenerateData.R

load("data/Result_Table.RDA")

# Create labels for our graphs by renaming
# the results column appropriately
my_labels = unlist(lapply(2:KMAX, function(i)
  paste("k =", i)))

# Convert the results table to a data frame
rt <- as.data.frame(result_table)
# Rename the columns
names(rt) <- my_labels
# Sort all columns independently so we can view
# the cumulants
rt.sorted <- as.data.frame(apply(rt,2,sort,decreasing=F))
# Assign a culmulant index
rt.sorted$index <- 1:N

# Convert the data frame to long form so it can
# be used easily with ggplot methods
d <- melt(rt.sorted, id.vars="index")

# Plot histograms with a fixed x axis so we can compare
ggplot(d, aes(value)) + facet_wrap(~variable, scales = 'free_y') +
  geom_histogram()

# Plot cumulants
ggplot(d, aes(index, value, col=variable)) + 
  geom_point(size=1) + coord_flip() + 
  xlab("Cumulative") + ylab("Similarity") +
  ggtitle(paste("Overlay of the Cumulative Distributions for\n",
          "Increasing Values of k", sep="")) +
  scale_colour_manual(labels=my_labels, 
                      values=rainbow(9))

@

\section{Choice of k}

If we trust our cumulative plot and histograms, we can see 
that \(k = 3\) is the optimal choice for our choice of clusters.  This
is the same choice as we determined in our previous lab given a less
rigorous method looking at within sum of squares vs. varying values of k.  


\section{Discuss whether you trust the method or not}

In general, I do trust this method.  It might not address how appropriate
k-means clustering is for a given problem, but, under the assumption that
k-means is in fact the correct method to use, this is a rigorous and 
quantitative way to deal with the question of which value of k to choose, 
and how stable that choice will be in the context of the underlying 
distribution.

\section{References}

Asa Ben-Hur, Andre Elisseeff, and Isabelle Guyon. A stability based method for discovering structure in clustered data. In Pacific symposium on biocomputing, volume 7, pages 6â€“17, 2001.

\section{Acknowledgements}

The author thanks Max Gardner for helpful discussions and assistance with this lab.

\end{document}